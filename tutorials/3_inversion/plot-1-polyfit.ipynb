{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "name": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.4.3",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPolyfit\n=======\n\nThis tutorial shows a flexible inversion with an own forward calculation that\nincludes an own jacobian. We start with fitting a polynomial of degree\n$P$\n\n\\begin{align}f(x) = p_0 + p_1 x + \\ldots + p_P x^P = \\sum\\limits_{i=0}^{P} p_i x^i\\end{align}\n\nto given data $y$.\nThe unknown model is the coefficient vector ${\\bf m}=[p_0,\\ldots,p_P]$.\nThe vectorized function for a vector\n${\\bf x}=[x_1,\\ldots,x_N]^T$\ncan be written as matrix-vector product\n\n\\begin{align}{\\bf f} ({\\bf x}) = {\\bf A} {\\bf x} \\quad\\mbox{with}\\quad {\\bf A}=\n  \\left[\n    \\begin{array}{cccc}\n        1 & x_1    & \\ldots & x_1^P \\\\\n   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        1 & x_N    & \\ldots & x_N^P\n  \\end{array}\n  \\right] =\n  [ {\\bf 1}\\quad {\\bf x} \\quad {\\bf x}^2 \\ldots {\\bf x}^P ] \\;.\\end{align}\n\nWe set up the modelling operator, i.e. to return ${\\bf f}({\\bf x})$ for\ngiven $p_i$, as a class derived from the modelling base class.\nThe latter holds the main mimic of generating Jacobian, gradients by brute\nforce. The only function to overwrite is \\cw{response()}.\n\nPython is a very flexible language for programming and scripting and has many\npackages for numerical computing and graphical visualization.\nFor this reason, we built Python bindings and compiled the library pygimli.\nAs a main advantage, all classes can be used and derived.\nThis makes the use of GIMLi very easy for non-programmers.\nAll existing modelling classes can be used, but it is also easy to create new\nmodelling classes.\n\nWe exemplify this by the preceding example.\n\nFirst, the library must be imported.\n\nTo avoid name clashes with other libraries we suggest to import `pygimli` and\nalias it to an easy name (as usually done for numpy or matplotlib), e.g. by\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import pygimli as pg\nimport numpy as np\nimport matplotlib.pyplot as plt"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The modelling class is derived from ModellingBase, a constructor is defined\nand the response function is defined. Due to the linearity of the problem we\nstore the matrix ${\\bf A}$, which is also the Jacobian matrix and use\nit for the forward calculation. A second function is just added as reference.\nWe overwrite the method createJacobian as we know it but do nothing in the\nactual computation. If ${\\bf J}$ depends on ${\\bf m}$ this\nfunction must be filled.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "class FunctionModelling(pg.ModellingBase):\n    def __init__(self, nc, xvec, verbose=False):\n        pg.ModellingBase.__init__(self, verbose)\n        self.x_ = xvec\n        self.nc_ = nc\n        nx = len(xvec)\n        self.regionManager().setParameterCount(nc)\n        self.jacobian().resize(nx, nc)\n        for i in range(self.nc_):\n            self.jacobian().setCol(i, pg.pow(self.x_, i))\n\n    def response(self, model):\n        return self.jacobian() * model\n\n    def responseDirect(self, model):\n        y = pg.RVector(len(self.x_), model[0])\n\n        for i in range(1, self.nc_):\n            y += pg.pow(self.x_, i) * model[i]\n\n        return y\n\n    def createJacobian(self, model):\n        pass  # if J depends on the model you should work here\n\n    def startModel(self):\n        return pg.RVector(self.nc_, 0.5)"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us create some synthetic data for some x values\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = np.arange(0., 10., 0.5)\ny = 1.1 + 2.1 * x - 0.2 * x**2\nnoise = 0.1\ny += np.random.randn(len(y)) * noise"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now start by setting up the modelling operator, and inversion and run it.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fop = FunctionModelling(3, x)\n\n# initialize inversion with data and forward operator and set options\ninv = pg.RInversion(y, fop)\n\n# constant absolute error of 0.01 is 1% (not necessary, only for chi^2)\ninv.setAbsoluteError(noise)\n\n# the problem is well-posed and does not need any regularization\ninv.setLambda(0)\n\n# actual inversion run yielding coefficient model\ncoeff = inv.run()\n\ninv.echoStatus()\n\nprint(coeff)"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result is easily plotted by\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plt.plot(x, y, 'rx', x, inv.response(), 'b-')\nplt.show()"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    }
  ]
}